{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Analysis Pipeline\n",
    "\n",
    "![](https://www.evernote.com/l/AAFtiWq4lUNNiqyswF72dUV3f_7Geq_TcykB/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install boto3 pymongo twitter --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Tweets to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tweet Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key    = \"MkzCxkb2dEjT0lJakO8aaYyNy\"\n",
    "consumer_secret = \"50gIenCSmtC6OVVhfr11KZ7RXuflqL5P4yIG6qWPft0lzg7yPS\"\n",
    "token           = \"494647099-hRiZkTdJDnqj3wGEfBv8rC8vhouzwhLc1hqvn9Wb\"\n",
    "token_secret    = \"IMqQHCjSyaBiuaR1xJStQaRiNM3Z6oiNcrXvSRzXckO4g\"\n",
    "bounding_box    = \"-118.5137323688,34.0001996344,-118.4702449172,34.0331651696\"\n",
    "\n",
    "tweet_iterator  = lib.create_tweet_iterator(token, \n",
    "                                            token_secret,\n",
    "                                            consumer_key,\n",
    "                                            consumer_secret,\n",
    "                                            bounding_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [next(tweet_iterator) for _ in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2 = []\n",
    "for _ in range(50):\n",
    "    tweets_2.append(next(tweet_iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write list of tweets to JSON file on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "username = 'chillsimmons'\n",
    "\n",
    "filename = lib.create_timestamped_filename(username)\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(tweets, outfile)\n",
    "    \n",
    "filename = lib.create_timestamped_filename(username)\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(tweets_2, outfile)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Tweet Files to S3, Step 1 - Create a Boto Client to S3\n",
    "\n",
    "https://boto3.readthedocs.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = lib.create_boto_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweets-chillsimmons-2018-07-27_00-53-46-422094.json',\n",
       " 'tweets-chillsimmons-2018-07-27_00-53-46-401750.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = listdir()\n",
    "current_directory = [file for file in current_directory if 'tweets-chillsimmons' in file]\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import rename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Tweet Files to S3, Step 2 - Write a file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = 'uclax-data-science'\n",
    "\n",
    "for filename in current_directory:\n",
    "    lib.write_file_to_S3(s3_client, filename, S3_BUCKET)\n",
    "    old_filename = filename\n",
    "    filename = filename.replace('.json', '')\n",
    "    filename = filename + '-processed.json'\n",
    "    rename(old_filename, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweets-chillsimmons-2018-07-27_00-53-46-401750-processed.json',\n",
       " 'tweets-chillsimmons-2018-07-27_00-53-46-422094-processed.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = listdir()\n",
    "current_directory = [file for file in current_directory if 'tweets-chillsimmons' in file]\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Files on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweets-chillsimmons-2018-07-27_00-53-46-401750.json',\n",
       " 'tweets-chillsimmons-2018-07-27_00-53-46-422094.json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_files = lib.list_files_in_S3_bucket(s3_client, S3_BUCKET)\n",
    "s3_files = [file for file in s3_files if 'tweets-chillsimmons' in file]\n",
    "s3_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Tweets to Mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read an object from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = 'uclax-data-science'\n",
    "key = s3_files[1]\n",
    "\n",
    "tweets_from_s3 = lib.read_object_from_S3(s3_client, key, S3_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Tweets to Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.errors import DuplicateKeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_client = lib.create_mongo_client_to_database_collection('twitter', 'tweets')\n",
    "for tweet in tweets_from_s3:\n",
    "    try:\n",
    "        collection_client.insert_one(tweet)\n",
    "    except DuplicateKeyError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee04376af00ba09c5a1') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee14376af00ba09c5a2') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee14376af00ba09c5a3') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee14376af00ba09c5a4') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee14376af00ba09c5a5') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee14376af00ba09c5a6') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee14376af00ba09c5a7') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee14376af00ba09c5a8') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee14376af00ba09c5a9') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee14376af00ba09c5aa') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5ab') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5ac') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5ad') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5ae') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5af') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5b0') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5b1') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5b2') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5b3') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5b4') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5b5') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5b6') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee24376af00ba09c5b7') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5b8') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5b9') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5ba') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5bb') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5bc') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5bd') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5be') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5bf') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5c0') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5c1') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5c2') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5c3') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5c4') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee34376af00ba09c5c5') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5c6') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5c7') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5c8') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5c9') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5ca') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5cb') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5cc') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5cd') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5ce') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5cf') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5d0') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5d1') }\n",
      "E11000 duplicate key error collection: twitter.tweets index: _id_ dup key: { : ObjectId('5b5a6ee44376af00ba09c5d2') }\n"
     ]
    }
   ],
   "source": [
    "collection_client = lib.create_mongo_client_to_database_collection('twitter', 'tweets')\n",
    "for tweet in tweets_from_s3:\n",
    "    try:\n",
    "        collection_client.insert_one(tweet)\n",
    "    except DuplicateKeyError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in s3_files[2:]:\n",
    "    for tweet in tweets_from_s3:\n",
    "        try:\n",
    "            collection_client.insert_one(tweet)\n",
    "        except DuplicateKeyError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One More Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [next(tweet_iterator) for _ in range(50)]\n",
    "\n",
    "filename = lib.create_timestamped_filename(username)\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(tweets, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweets-chillsimmons-2018-07-27_01-06-45-304032.json']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = listdir()\n",
    "current_directory = [file for file in current_directory \n",
    "                     if ('tweets-chillsimmons' in file) and ('processed' not in file)]\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in current_directory:\n",
    "    lib.write_file_to_S3(s3_client, filename, S3_BUCKET)\n",
    "    old_filename = filename\n",
    "    filename = filename.replace('.json', '')\n",
    "    filename = filename + '-processed.json'\n",
    "    rename(old_filename, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweets-chillsimmons-2018-07-27_00-53-46-401750.json',\n",
       " 'tweets-chillsimmons-2018-07-27_00-53-46-422094.json',\n",
       " 'tweets-chillsimmons-2018-07-27_01-06-45-304032.json']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_files = lib.list_files_in_S3_bucket(s3_client, S3_BUCKET)\n",
    "s3_files = [file for file in s3_files if 'tweets-chillsimmons' in file]\n",
    "s3_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tweets-chillsimmons-2018-07-27_01-06-45-304032.json'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = s3_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_from_s3 = lib.read_object_from_S3(s3_client, key, S3_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f1b43365988>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_client.insert_many(tweets_from_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN           = \"494647099-hRiZkTdJDnqj3wGEfBv8rC8vhouzwhLc1hqvn9Wb\"\n",
    "TOKEN_SECRET    = \"IMqQHCjSyaBiuaR1xJStQaRiNM3Z6oiNcrXvSRzXckO4g\"\n",
    "CONSUMER_KEY    = \"MkzCxkb2dEjT0lJakO8aaYyNy\"\n",
    "CONSUMER_SECRET = \"50gIenCSmtC6OVVhfr11KZ7RXuflqL5P4yIG6qWPft0lzg7yPS\"\n",
    "BOUNDING_BOX    = \"-118.5137323688,34.0001996344,-118.4702449172,34.0331651696\"\n",
    "\n",
    "DATABASE        = 'twitter'\n",
    "COLLECTION      = 'tweets'\n",
    "USERNAME        = 'chillsimmons'\n",
    "\n",
    "S3_BUCKET       = 'uclax-data-science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_iterator    = lib.create_tweet_iterator(TOKEN, TOKEN_SECRET, \n",
    "                                              CONSUMER_KEY, CONSUMER_SECRET, BOUNDING_BOX)\n",
    "s3_client         = lib.create_boto_client()\n",
    "collection_client = lib.create_mongo_client_to_database_collection(DATABASE, COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    now = datetime.now().strftime('%D %H:%M:%S')\n",
    "    print(now, end=' | ')\n",
    "    print('Collecting Tweets', end=' | ')\n",
    "    \n",
    "def collect_tweets(n=50):\n",
    "    tweets = [next(tweet_iterator) for _ in range(n)]\n",
    "    print('{} Tweets'.format(n), end=' | ')\n",
    "    return tweets\n",
    "    \n",
    "def write_to_disk(tweets):\n",
    "    filename = lib.create_timestamped_filename(USERNAME) \n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(tweets, outfile)    \n",
    "    print('Written to Disk', end=' | ')\n",
    "    return filename\n",
    "    \n",
    "def write_to_S3(filename):   \n",
    "    lib.write_file_to_S3(s3_client, filename, S3_BUCKET)\n",
    "    old_filename = filename\n",
    "    filename = filename.replace('.json', '')\n",
    "    filename = filename + '-processed.json'\n",
    "    rename(old_filename, filename)\n",
    "    print('Written to S3', end=' | ')\n",
    "    \n",
    "def insert_to_mongo(key):\n",
    "    tweets_from_s3 = lib.read_object_from_S3(s3_client, key, S3_BUCKET)    \n",
    "    collection_client.insert_many(tweets_from_s3)\n",
    "    print('Inserted to Mongo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/27/18 01:22:04 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:22:08 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:23:05 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:23:57 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:25:14 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:25:51 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:26:39 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:27:42 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:28:39 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:29:40 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:30:41 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:31:36 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:32:28 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:33:19 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:34:16 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:35:12 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:36:05 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:37:11 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:38:00 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:39:06 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:40:20 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:41:13 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:42:13 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:43:13 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:44:11 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:45:44 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:46:16 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:47:04 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:48:26 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:54:38 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:54:39 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:54:39 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:54:40 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:54:40 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:54:41 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:54:41 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:55:04 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:55:56 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:56:54 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:57:53 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:58:48 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 01:59:34 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:00:26 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:01:20 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:02:34 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:03:31 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:04:20 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:05:39 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:06:33 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:07:37 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:08:38 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:09:37 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:10:25 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:11:28 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:12:24 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:13:24 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:14:31 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:15:36 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:16:40 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:17:30 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:18:22 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:19:42 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:20:44 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:21:38 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:22:44 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:24:08 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:25:17 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:26:23 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:27:13 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:28:37 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:29:34 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:30:40 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:31:43 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:32:54 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:33:45 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:34:42 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:35:34 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:36:34 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/27/18 02:37:38 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:38:47 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:39:48 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:41:12 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:42:12 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:43:10 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:44:01 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:44:59 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:45:57 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:46:43 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:47:32 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:48:21 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:49:23 | Collecting Tweets | 50 Tweets | Written to Disk | Written to S3 | Inserted to Mongo\n",
      "07/27/18 02:50:27 | Collecting Tweets | "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    timestamp()\n",
    "    tweets = collect_tweets()\n",
    "    filename = write_to_disk(tweets)\n",
    "    write_to_S3(filename)\n",
    "    insert_to_mongo(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
